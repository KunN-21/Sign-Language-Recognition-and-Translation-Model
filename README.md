# Sign Language Recognition System

## ğŸ“– Tá»•ng quan dá»± Ã¡n

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng nháº­n dáº¡ng ngÃ´n ngá»¯ kÃ½ hiá»‡u (ASL - American Sign Language) trong thá»i gian thá»±c sá»­ dá»¥ng computer vision vÃ  machine learning. Há»‡ thá»‘ng cÃ³ thá»ƒ nháº­n dáº¡ng cÃ¡c kÃ½ hiá»‡u báº±ng tay thÃ´ng qua webcam vÃ  Ä‘Æ°a ra dá»± Ä‘oÃ¡n vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao.

## ğŸ¯ Má»¥c tiÃªu

- Nháº­n dáº¡ng cÃ¡c kÃ½ hiá»‡u ngÃ´n ngá»¯ kÃ½ hiá»‡u ASL trong thá»i gian thá»±c
- XÃ¢y dá»±ng giao diá»‡n ngÆ°á»i dÃ¹ng thÃ¢n thiá»‡n vá»›i PyQt5
- Äáº¡t Ä‘á»™ chÃ­nh xÃ¡c cao vá»›i tá»‘c Ä‘á»™ xá»­ lÃ½ nhanh
- Tá»‘i Æ°u hÃ³a cho viá»‡c triá»ƒn khai thá»±c táº¿

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera Input  â”‚ -> â”‚ MediaPipe Hands  â”‚ -> â”‚ Hand Landmarks  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prediction    â”‚ <- â”‚   ML Classifier  â”‚ <- â”‚ Preprocessing   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ PhÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n

### Sá»­ dá»¥ng MediaPipe thay vÃ¬ CNN

#### 1. **Hiá»‡u quáº£ tÃ­nh toÃ¡n**
- **MediaPipe + Classifier**: Chá»‰ cáº§n trÃ­ch xuáº¥t 21 landmarks (63 features) â†’ Model nhá» gá»n
- **CNN**: Xá»­ lÃ½ toÃ n bá»™ image (224x224x3 = 150,528 features) â†’ TÃ­nh toÃ¡n phá»©c táº¡p

#### 2. **Báº¥t biáº¿n vá»›i mÃ´i trÆ°á»ng**
- MediaPipe chuáº©n hÃ³a tá»a Ä‘á»™ landmark vá» [0,1] â†’ KhÃ´ng phá»¥ thuá»™c background, lighting
- CNN dá»… bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi background noise, Ã¡nh sÃ¡ng, mÃ u sáº¯c

#### 3. **Tá»‘c Ä‘á»™ FPS cao**
- MediaPipe: ~30-60 FPS trÃªn CPU thÆ°á»ng
- CNN: Cáº§n GPU máº¡nh Ä‘á»ƒ Ä‘áº¡t real-time

#### 4. **PhÃ¹ há»£p vá»›i dá»¯ liá»‡u training nhá» vÃ  vá»«a**
- Landmark features cÃ³ tÃ­nh invariant cao â†’ Cáº§n Ã­t data hÆ¡n
- CNN cáº§n dataset lá»›n Ä‘á»ƒ há»c Ä‘Æ°á»£c cÃ¡c pattern phá»©c táº¡p

#### 5. **Robustness**
```python
# MediaPipe chuáº©n hÃ³a tá»a Ä‘á»™
normalized_landmarks = [(x/image_width, y/image_height, z) for x,y,z in landmarks]
# â†’ Báº¥t biáº¿n vá»›i kÃ­ch thÆ°á»›c áº£nh, vá»‹ trÃ­ tay trong frame
```

## ğŸ“Š Quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u

### 1. **Data Augmentation**
```python
# preprocessing/augment.py
transform = A.Compose([
    A.HorizontalFlip(p=0.5),              # Láº­t ngang
    A.ShiftScaleRotate(
        shift_limit=0.10,                 # Dá»‹ch chuyá»ƒn Â±10%
        scale_limit=0.10,                 # Scale Â±10%
        rotate_limit=20,                  # Xoay Â±20Â°
        p=0.8
    ),
    A.RandomBrightnessContrast(p=0.7),    # Thay Ä‘á»•i sÃ¡ng/tÆ°Æ¡ng pháº£n
    A.GaussianBlur(blur_limit=(3, 7), p=0.5), # LÃ m má» nháº¹
])
```

**Táº¡i sao augment?**
- TÄƒng kÃ­ch thÆ°á»›c dataset tá»« N â†’ NÃ—(1+5) = 6N samples
- TÄƒng tÃ­nh Ä‘a dáº¡ng: gÃ³c nhÃ¬n, Ã¡nh sÃ¡ng, vá»‹ trÃ­ khÃ¡c nhau
- Giáº£m overfitting, tÄƒng generalization

### 2. **Landmark Extraction**
```python
# preprocessing/extract_landmark.py
def enhance_brightness_contrast(image):
    """CLAHE (Contrast Limited Adaptive Histogram Equalization)"""
    img_y_cr_cb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(img_y_cr_cb)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    y = clahe.apply(y)
    img_y_cr_cb = cv2.merge((y, cr, cb))
    return cv2.cvtColor(img_y_cr_cb, cv2.COLOR_YCrCb2BGR)
```

**Pipeline xá»­ lÃ½:**
1. **Image Enhancement**: CLAHE Ä‘á»ƒ cáº£i thiá»‡n tÆ°Æ¡ng pháº£n
2. **Hand Detection**: MediaPipe Hands detection
3. **Landmark Extraction**: 21 Ä‘iá»ƒm landmark Ã— 3 tá»a Ä‘á»™ (x,y,z) = 63 features
4. **CSV Export**: LÆ°u features + labels Ä‘á»ƒ training

### 3. **Feature Engineering**

**Äáº·c trÆ°ng Landmark:**
- **21 keypoints** cá»§a bÃ n tay tá»« MediaPipe
- **Tá»a Ä‘á»™ (x,y,z)**: x,y Ä‘Ã£ normalize [0,1], z lÃ  depth
- **Invariant properties**: KhÃ´ng phá»¥ thuá»™c kÃ­ch thÆ°á»›c áº£nh, vá»‹ trÃ­ tay

```
Thumb: 0-4    â”‚ Index: 5-8     â”‚ Middle: 9-12   â”‚ Ring: 13-16    â”‚ Pinky: 17-20
   4             8                12               16               20
   â”‚             â”‚                â”‚                â”‚                â”‚
   3             7                11               15               19
   â”‚             â”‚                â”‚                â”‚                â”‚
   2             6                10               14               18
   â”‚             â”‚                â”‚                â”‚                â”‚
   1             5                9                13               17
   â”‚            â•±                â•±                â•±                â•±
   0 â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±
 (wrist)
```

## ğŸ¤– Model Architecture & Training

### Model Design
```python
model = Sequential([
    Dense(256, activation='tanh', input_shape=(63,)),  # Input: 21Ã—3 landmarks
    Dense(128, activation='tanh'),                     # Hidden layer
    Dense(num_classes, activation='softmax')           # Output: Class probabilities
])
```

**Táº¡i sao architecture nÃ y?**
- **Dense layers**: PhÃ¹ há»£p vá»›i tabular data (landmarks)
- **Tanh activation**: Hiá»‡u quáº£ vá»›i normalized input [-1,1]
- **Softmax output**: Multi-class classification
- **Compact**: Chá»‰ ~100K parameters â†’ Fast inference

### Training Strategy
```python
# Callbacks for optimal training
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,                    # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
    restore_best_weights=True       # Láº¥y weights tá»‘t nháº¥t
)

model_ckpt = ModelCheckpoint(
    "best_sign_classifier.keras",
    monitor='val_loss',
    save_best_only=True            # Chá»‰ lÆ°u model tá»‘t nháº¥t
)
```

### Preprocessing Pipeline
```python
# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Chuáº©n hÃ³a features vá» mean=0, std=1

# Label encoding
le = LabelEncoder()
y_encoded = le.fit_transform(labels)  # String labels â†’ Integer
y_categorical = to_categorical(y_encoded)  # One-hot encoding
```

## ğŸš€ Real-time Inference

### Optimizations trong Real-time
```python
# Smoothing predictions vá»›i voting
BUFFER_SIZE = 15
DEBOUNCE = 4
predictions_buffer = deque(maxlen=BUFFER_SIZE)

# Majority voting Ä‘á»ƒ giáº£m noise
def get_stable_prediction(buffer):
    if len(buffer) >= DEBOUNCE:
        counter = Counter(buffer)
        most_common = counter.most_common(1)[0]
        if most_common[1] >= DEBOUNCE:
            return most_common[0]
    return None
```

**Ká»¹ thuáº­t tá»‘i Æ°u:**
1. **Temporal Smoothing**: Voting trÃªn 15 frames gáº§n nháº¥t
2. **Debouncing**: Chá»‰ accept prediction khi xuáº¥t hiá»‡n â‰¥4 láº§n
3. **Single Hand**: max_num_hands=1 Ä‘á»ƒ tÄƒng tá»‘c
4. **Confidence Threshold**: min_detection_confidence=0.5

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
SignLanguage/
â”œâ”€â”€ Model/
â”‚   â”œâ”€â”€ Model.ipynb              # Training notebook
â”‚   â”œâ”€â”€ best_sign_classifier.keras  # Trained model
â”‚   â”œâ”€â”€ sign_label_encoder.pkl   # Label encoder
â”‚   â””â”€â”€ sign_scaler.pkl          # Feature scaler
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ extract_landmark.py      # Landmark extraction
â”‚   â”œâ”€â”€ augment.py              # Data augmentation
â”‚   â””â”€â”€ visualize_landmark.py   # Visualization tools
â”œâ”€â”€ data/
â”‚   â””â”€â”€ hand_landmarks.csv      # Processed dataset
â”œâ”€â”€ ASL/                        # Additional ASL data
â”œâ”€â”€ realtime.py                 # Main real-time app
â””â”€â”€ README.md                   # This file
```

## ğŸ® CÃ¡ch sá»­ dá»¥ng

### 1. CÃ i Ä‘áº·t dependencies
```bash
pip install opencv-python mediapipe tensorflow scikit-learn PyQt5 pyttsx3 albumentations joblib
```

### 2. Training model (náº¿u cáº§n)
```bash
jupyter notebook Model/Model.ipynb
```

### 3. Cháº¡y á»©ng dá»¥ng real-time
```bash
python realtime.py
```

## ğŸ“¹ Demo Video

*[Video demo sáº½ Ä‘Æ°á»£c thÃªm vÃ o sau]*

Trong video demo, báº¡n sáº½ tháº¥y:
- Nháº­n dáº¡ng real-time cÃ¡c kÃ½ hiá»‡u ASL
- Giao diá»‡n PyQt5 vá»›i visualization landmarks
- Äá»™ chÃ­nh xÃ¡c vÃ  tá»‘c Ä‘á»™ xá»­ lÃ½
- Text-to-speech feedback

## ğŸ”§ Ká»¹ thuáº­t nÃ¢ng cao

### 1. **Data Preprocessing**
- **CLAHE Enhancement**: Cáº£i thiá»‡n contrast cho Ä‘iá»u kiá»‡n Ã¡nh sÃ¡ng khÃ¡c nhau
- **Landmark Normalization**: MediaPipe tá»± Ä‘á»™ng normalize vá» [0,1]
- **Missing Data Handling**: Skip images khÃ´ng detect Ä‘Æ°á»£c tay

### 2. **Model Optimization**
- **Early Stopping**: TrÃ¡nh overfitting
- **Stratified Split**: Äáº£m báº£o balance classes trong train/test
- **Feature Scaling**: StandardScaler cho convergence nhanh hÆ¡n

### 3. **Real-time Optimization**
- **Temporal Filtering**: Giáº£m noise prediction
- **Multi-threading**: UI khÃ´ng bá»‹ block khi inference
- **Memory Management**: Circular buffer cho predictions

## ğŸ“Š Káº¿t quáº£ vÃ  ÄÃ¡nh giÃ¡

### Metrics Ä‘áº¡t Ä‘Æ°á»£c:
- **Accuracy**: ~95%+ trÃªn test set
- **Inference Speed**: 30+ FPS trÃªn CPU
- **Model Size**: <5MB
- **Real-time Latency**: <50ms per frame

### So sÃ¡nh vá»›i CNN approach:
| Metric | MediaPipe + ML | CNN Direct |
|--------|---------------|------------|
| Model Size | ~5MB | ~50-200MB |
| Training Time | ~10 mins | ~2-5 hours |
| Inference Speed | 30+ FPS (CPU) | 15-30 FPS (GPU) |
| Data Required | ~1K samples/class | ~5K+ samples/class |
| Robustness | High (invariant) | Medium (background dependent) |

## ğŸ”® HÆ°á»›ng phÃ¡t triá»ƒn

1. **Multi-hand Support**: Há»— trá»£ nháº­n dáº¡ng 2 tay
2. **Continuous Recognition**: Nháº­n dáº¡ng cÃ¢u/tá»« liÃªn tá»¥c
3. **3D Pose Integration**: ThÃªm body pose cho context
4. **Mobile Deployment**: Port sang TensorFlow Lite
5. **Custom Vocabulary**: ThÃªm kÃ½ hiá»‡u Viá»‡t Nam

## ğŸ‘¥ ÄÃ³ng gÃ³p

Dá»± Ã¡n má»Ÿ Ä‘á»ƒ há»c táº­p vÃ  nghiÃªn cá»©u. Má»i Ä‘Ã³ng gÃ³p vÃ  feedback Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh!

## ğŸ“„ License

MIT License - Tá»± do sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

---

*Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn vá»›i má»¥c Ä‘Ã­ch há»c táº­p vá» Computer Vision vÃ  Machine Learning.*
